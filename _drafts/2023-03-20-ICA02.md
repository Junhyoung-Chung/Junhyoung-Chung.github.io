---
title: 02-ICA
tags:
    - Statistics
    - Statistics-etc
    - Papers
---

이제부터는 본격적인 수학적 원리에 대해 살펴보도록 하겠다.

<!--more-->

---

## 4. Principles of ICA estimation

### 4.1. "Non-Gaussian is independent"

$$
y = \mathbf{w}^T\mathbf{x}
$$

를 생각해보자. 여기서 만약 $\mathbf{w}$가 $\mathbf{A}^{-1}$의 row 중 하나라면 $y$는 indpendent component인 $\mathbf{s}$ 중 하나의 element가 될 것이다. 하지만 문제는 우리가 $\mathbf{A}$에 대한 정보가 없다는 것이다. 어떻게 하면 $\mathbf{w}$을 $\mathbf{A}^{-1}$의 row 중 하나와 같게끔 approximation을 할 수 있을까?

$$
\mathbf{z} = \mathbf{A}^T\mathbf{w}
$$

를 정의하자. 즉,

$$
y = \mathbf{w}^T\mathbf{x} = \mathbf{w}^T\mathbf{As} = \mathbf{z}^T\mathbf{s}
$$

가 되어 $y$는 $\mathbf{s}$의 선형결합으로 이루어지는 것을 확인할 수 있다.

우리는 여기서 non-Gaussianity 가정을 활용한다.

Central Limit Theorem에 따르면 i.i.d.인 확률변수들의 합은 점점 정규분포로 수렴한다. 결국 $\mathbf{z}^T\mathbf{s}$가 가장 정규분포와 멀어지기 위해서는 $\mathbf{z}$에서 오직 하나의 element만이 non-zero 값을 가지는 경우가 될 것이다. 즉, $\mathbf{w}^T\mathbf{x} = \mathbf{z}^T\mathbf{s}$가 independent component 중 하나에 대응된다.

즉, non-Gaussianity를 극대화하는 과정에서 우리는 independent component들을 얻을 수 있게 되는 것이다. 실제로 non-Gaussianity를 극대화하는 과정에서 $n$차원의 벡터 $\mathbf{w}$ $2n$개의 local maxima를 갖게 되는데, $s_i,-s_i$ 두 개씩 있기 때문이다.

non-Gaussianity를 평가하기 위한 metric 중 몇 가지를 소개한다.

#### 4.2.1. Kurtosis

Kurtosis는 "첨도"를 뜻한다. 확률분포의 꼬리가 얼마나 두터운지를 나타내는 척도이다. 다음과 같이 정의하자.

$$
\text{kurt}(y) = E(y^4)-3(E(y^2))^2 
$$

우리는 $E(y^2)=1$이라는 가정을 하고 있기 때문에

$$
\text{kurt}(y) = E(y^4) - 3
$$

이다. 여기서 만약 $y$가 정규분포를 따른다면 $E(y^4) = 3(E(y^2))^2$임을 쉽게 보일 수 있는데, 따라서 이 경우 $y$의 kurtosis는 0이다.

그리고 kurtosis는 다음과 같은 두 가지 성질을 가진다.

$$
\text{kurt}(x_1+x_2) = \text{kurt}(x_1) + \text{kurt}(x_2) \quad x_1 \perp x_2
$$

$$
\text{kurt}(\alpha x_1) = \alpha^4 \text{kurt}(x_1)
$$

즉,

$$
y = \mathbf{z}^T\mathbf{s} = z_1s_1+z_2s_2
$$

라고 할 때

$$
\text{kurt}(y) = z_1^4\text{kurt}(s_1) + z_2^4\text{kurt}(s_2)
$$

가 되는 것이다. 따라서,

$$
\max\{z_1^4\text{kurt}(s_1) + z_2^4\text{kurt}(s_2)\} \quad \text{s.t.} \quad z_1^2 + z_2^2 = 1
$$

문제를 풀게 된다. 여기서 제약식은 $E(y^2)=1$이라는 가정으로부터 나온다.

하지만 kurtosis는 이상점에 굉장히 취약하다는 단점이 있기 때문에 practical하지 못하다.

#### 4.2.2. Negentropy

Entropy라는 또 다른 measure가 있다. 우리가 관심있는 부분은 연속확률변수에 한정되어 있기 떄문에 그 부분만 살펴보자.

$$
H(\mathbf{y}) = -\int f(\mathbf{y})\log f(\mathbf{y}) d\mathbf{y}
$$

Entropy는 information theory에서 중요하게 쓰이는 개념이다. 어떤 확률변수가 갖고 있는 정보량을 나타내는 척도 정도로 생각하면 되겠다. 변수가 예측불가능하고 구조를 파악할 수 없다면 Entropy는 크게 나타난다. 즉, "랜덤"할수록 Entropy는 커진다.

그리고 평균이 $\mu$, 분산이 $\Sigma$ 로 동일하고 실수 전체를 집합으로 가지는 확률변수들 중에서는 **gaussian random variable이 가장 Entropy가 높다는 것이 증명되었다.**
따라서 Entropy 역시 non-Gaussianity를 나타내는 척도가 될 것이다.

Negentropy는 아래와 같이 정의한다.

$$
J(\mathbf{y}) = H(\mathbf{y}_{\text{gauss}}) - H(\mathbf{y})
$$

여기서 $J(\mathbf{y})$는 항상 non-negative한 값을 가지고, non-Gaussianity의 정도가 클수록 negentropy는 큰 값을 가질 것이다.

#### 4.2.3. Approximations of negentropy

하지만 공식을 보면 알 수 있듯, 엔트로피를 구하기 위해서는 확률밀도함수를 알아야 하기 때문에 함수를 추정해야 하는 번거로움이 있다. 이를 해결하기 위해 다음과 같은 approximation 방법이 제안되어 있다.



## Reference

* <a href="https://www.sciencedirect.com/science/article/pii/S0893608000000265">Independent component analysis: algorithms and applications</a>