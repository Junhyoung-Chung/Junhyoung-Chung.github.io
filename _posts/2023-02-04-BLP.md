---
title: Best Linear Predictor
tags:
    - Statistics
    - Statistics-etc
---

두 확률변수 $X_1$, $X_2$ 가 있다고 가정해보자. 이 때 $X_2$을 통해 $X_1$을 가장 잘 predict할 수 있는 선형 조합은 무엇일까?

<!--more-->

## Definition

우선 "가장 잘"이 무엇인지에 대해 정의할 필요가 있다. 여러가지 정의가 있을 수 있지만 통상적으로 우리가 알고 있는 Best Linear Predictor을 유도하기 위해서는 아래의 정의를 따라야 한다.

$$
E\lvert X_1-\text{BLP}(X_2;X_1) \rvert^2=\min_{A,b}{E\lvert X_1-(AX_2+b)\rvert^2}
$$

즉, $E\lvert X_1-(AX_2+b)\rvert^2$를 최소로 하는 $A^\ast X_2+b^\ast$가 바로 $X_1$의 **Best Linear Predictor**라는 뜻이다. 결국 $X_1$과 거리가 가장 가까운 $X_2$의 선형조합을 찾겠다는 의미이다.

## Claim

이제 다음과 같은 주장을 하려 한다.

$$
\left[
\begin{matrix}
    X_1
    \newline X_2
\end{matrix}
\right] \sim \left( \left[
\begin{matrix}
    \mu_1
    \newline \mu_2
\end{matrix}
\right] , \left[
\begin{matrix}
    \Sigma_{11} & \Sigma_{12}
    \newline \Sigma_{21} & \Sigma_{22}
\end{matrix}
\right]  \right)
$$

일 때,

$$
\text{BLP}(X_2;X_1)=\mu_1+\Sigma_{12}\Sigma_{22}^{-1}(X_2-\mu_2)
$$

## Proof

우선

$$
E\lvert X_1-(AX_2+b)\rvert^2 = E\lvert X_1-\text{BLP}(X_2;X_1)+\text{BLP}(X_2;X_1)-(AX_2+b)\rvert^2
$$

로 고쳐 써보자.

그 다음,

$$
W_1 =  X_1-\text{BLP}(X_2;X_1)
$$

$$
W_2 = AX_2+b-\text{BLP}(X_2;X_1)
$$

라 두면

$$
E\lvert X_1-(AX_2+b)\rvert^2 = E\lvert W_1-W_2 \rvert^2
$$

가 된다.

이제 $E\lvert W_1-W_2 \rvert^2 = E\lvert W_1 \rvert^2 + E\lvert W_2 \rvert^2$ 임을 보이려고 한다.

(7)번 식을 전개하면

$$
\begin{aligned}
E\lvert W_1-W_2 \rvert^2 & = E(W_1-W_2)^T(W_1-W_2)
\newline & = E(W_1^TW_1)+E(W_2^TW_2)-E(W_2^TW_1)-E(W_1^TW_2)
\newline & = E\lvert W_1 \rvert^2 + E\lvert W_2 \rvert^2 -E(W_2^TW_1)-E(W_1^TW_2) 
\end{aligned}
$$

여기서 $E(W_2^TW_1)=E(W_1^TW_2)=0$ 임을 보이면 된다.

$$
W_1 = CX_2+d
$$

로 잠깐 치환하고 문제를 풀어보도록 하자.

$$
\begin{aligned}
E(W_1^TW_2) & = E(X_1-\mu_1-\Sigma_{12}\Sigma_{22}^{-1}(X_2-\mu_2))^T(CX_2+d)
\newline & = E(X_1-\mu_1-\Sigma_{12}\Sigma_{22}^{-1}(X_2-\mu_2))^TCX_2+E(X_1-\mu_1-\Sigma_{12}\Sigma_{22}^{-1}(X_2-\mu_2))^Td
\newline & = E(X_1-\mu_1-\Sigma_{12}\Sigma_{22}^{-1}(X_2-\mu_2))^TCX_2
\newline & = E(tr((X_1-\mu_1-\Sigma_{12}\Sigma_{22}^{-1}(X_2-\mu_2))^TCX_2))
\newline & = E(tr(CX_2(X_1-\mu_1-\Sigma_{12}\Sigma_{22}^{-1}(X_2-\mu_2))^T))
\newline & = tr(CEX_2(X_1-\mu_1-\Sigma_{12}\Sigma_{22}^{-1}(X_2-\mu_2))^T)
\newline & = tr[C(E(X_2X_1^T)-\mu_2\mu_1^T-EX_2(X_2-\mu_2)^T\Sigma_{22}^{-1}\Sigma_{21})]
\newline & = tr[C(\Sigma_{21}-\Sigma_{22}\Sigma_{22}^{-1}\Sigma_{21})]
\newline & = tr[C(\Sigma_{21}-\Sigma_{21})]
\newline & = 0
\end{aligned}
$$

$E(W_2^TW_1)$인 경우에도 마찬가지로 $0$과 같음을 보일 수 있다.

따라서,

$$
\begin{aligned}
E\lvert W_1-W_2 \rvert^2 & = E\lvert W_1 \rvert^2 + E\lvert W_2 \rvert^2
\newline & \geq E\lvert W_1 \rvert^2 \quad (\text{equality holds when }W_2=0)
\end{aligned}
$$

따라서, 

$$
A^\ast = \Sigma_{12}\Sigma_{22}^{-1}
$$

$$
b^\ast = \mu_1-\Sigma_{12}\Sigma_{22}^{-1}\mu_2
$$

가 된다. $\blacksquare$

## Advanced

만약, 

$$
\left[
\begin{matrix}
    X_1
    \newline X_2
\end{matrix}
\right] \sim N\left( \left[
\begin{matrix}
    \mu_1
    \newline \mu_2
\end{matrix}
\right] , \left[
\begin{matrix}
    \Sigma_{11} & \Sigma_{12}
    \newline \Sigma_{21} & \Sigma_{22}
\end{matrix}
\right]  \right)
$$

일 때에는

$$
EX_1 \vert X_2 = \mu_1+\Sigma_{12}\Sigma_{22}^{-1}(X_2-\mu_2)
$$

임을 보일 수가 있다. 즉, 정규분포 하에서는 조건부기댓값이 바로 BLP 라는 것이다!

$$
\left[
\begin{matrix}
    I & -\Sigma_{12}\Sigma_{22}^{-1}
    \newline 0 & I
\end{matrix}
\right] \left[
\begin{matrix}
    X_1-\mu_1
    \newline X_2-\mu_2
\end{matrix}
\right]
$$

의 분포를 계산해보면 쉽게 확인할 수 있다.

## Reference

* 김우철. (2022). 개정판 수리통계학(pp.179~184). 민영사.